{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow-text\n!pip install transformers\n!pip install fasttext\n!pip install scipy\n!pip3 install \"scikit_learn==0.22.2.post1\"\n!pip install tweepy==4.5.0\n!pip install sentence-transformers\n!pip install wikipedia\n!pip install sentencepiece","metadata":{"execution":{"iopub.status.busy":"2023-05-21T18:22:05.726685Z","iopub.execute_input":"2023-05-21T18:22:05.727144Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"^C\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.12.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.8.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: fasttext in /opt/conda/lib/python3.7/site-packages (0.9.2)\nRequirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from fasttext) (59.8.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fasttext) (1.21.6)\nRequirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.7/site-packages (from fasttext) (2.10.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (1.7.3)\nRequirement already satisfied: numpy<1.23.0,>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from scipy) (1.21.6)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install python-telegram-bot --upgrade","metadata":{"execution":{"iopub.status.busy":"2023-05-20T10:33:31.878591Z","iopub.execute_input":"2023-05-20T10:33:31.878919Z","iopub.status.idle":"2023-05-20T10:36:01.874121Z","shell.execute_reply.started":"2023-05-20T10:33:31.878885Z","shell.execute_reply":"2023-05-20T10:36:01.872632Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7b3880314990>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/python-telegram-bot/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7b3880314610>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/python-telegram-bot/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7b3880350050>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/python-telegram-bot/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7b3880350390>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/python-telegram-bot/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7b38803506d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/python-telegram-bot/\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement python-telegram-bot (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for python-telegram-bot\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import requests\nimport base64\nimport json\nimport logging\nimport pandas as pd\nfrom telegram.ext import Updater , CommandHandler , MessageHandler , Filters\nfrom fastai.vision.all import load_learner\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nfrom transformers import pipeline , AutoTokenizer, TFAutoModelForSequenceClassification\nimport fasttext\nimport numpy as np\nimport pickle\nimport spacy\nfrom spacy import displacy\nfrom collections import Counter\nimport en_core_web_sm\nfrom pprint import pprint\nimport wikipedia as wiki\nfrom nltk.util import ngrams\nimport gensim\nimport re\nimport nltk\nimport torch\nfrom nltk import tokenize\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sentence_transformers import SentenceTransformer\nimport tweepy as tweepy\n\nnltk.download('stopwords')\nnltk.download('omw-1.4')\nnltk.download('punkt')\nnltk.download('wordnet')\n\nstop_words = set(stopwords.words('english'))","metadata":{"id":"kQ-_gg6f26ix","execution":{"iopub.status.busy":"2023-05-20T09:57:07.493459Z","iopub.status.idle":"2023-05-20T09:57:07.494671Z","shell.execute_reply.started":"2023-05-20T09:57:07.494385Z","shell.execute_reply":"2023-05-20T09:57:07.494411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2023-05-20T09:57:07.496269Z","iopub.status.idle":"2023-05-20T09:57:07.497154Z","shell.execute_reply.started":"2023-05-20T09:57:07.496886Z","shell.execute_reply":"2023-05-20T09:57:07.496922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"APIKey = \"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\"\n!wget -O ./lid.176.bin https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\nCONSUMER_KEY = \"eJL1xOgPnXVx0DzCr5pGa8lNv\"\nCONSUMER_SECRET = \"iDBuPdCEXZQDzsRqvNtkVcIhcdvlT8x8aW74VTm1EqXcIaPmrZ\"\nOAUTH_TOKEN = \"1420293020080082948-Qo8PBaf5oXA1xrPryabo3C3g09xdBf\"\nOAUTH_TOKEN_SECRET = \"HzW0KllX3NRls7pOKA0cPIbNEyAFWON9wgVODcRwlrVBi\"\nBEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAABKRewEAAAAAem8kRGNx2U5tGTmD%2BtikqmENETI%3DCqzEWLXGEmM8WKNXRRnW0Tke4QlWw2sihgwtjpVYwnAR0QD6bo\"\ntwitterAPI = tweepy.Client(bearer_token = BEARER_TOKEN)","metadata":{"id":"ultiAIXB50mk","execution":{"iopub.status.busy":"2023-05-20T09:57:07.498794Z","iopub.status.idle":"2023-05-20T09:57:07.499269Z","shell.execute_reply.started":"2023-05-20T09:57:07.499047Z","shell.execute_reply":"2023-05-20T09:57:07.499068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def start(update, context) :\n    update.message.reply_text(\n        \"EN : Just give me a news and I will tell you whether it is FAKE or not\"\n    )\n\ndef help_command(update, context) :\n    update.message.reply_text('My only purpose is to tell you whether a given news is fake or not')\n\ndef load_models() :\n    global clickBaitModel , sentimentModel , biasModel , classification_after_embedding_model, cae_model_tokenizer, liar_classification_model, wikipedia_model, nlp, d2v_model\n    classification_after_embedding_model = TFAutoModelForSequenceClassification.from_pretrained('pururaj/Test_model')\n    cae_model_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n    clickBaitModel = pipeline(model=\"elozano/bert-base-cased-clickbait-news\", tokenizer=\"elozano/bert-base-cased-clickbait-news\")\n    sentimentModel = pipeline(model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\", tokenizer=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n    biasModel = pipeline(model=\"d4data/bias-detection-model\", tokenizer=\"d4data/bias-detection-model\")\n    liar_classification_model = pickle.load(open('../input/inputdata/liar_classification.sav', 'rb'))\n    wikipedia_model = SentenceTransformer('bert-base-nli-mean-tokens')\n    nlp = en_core_web_sm.load()\n    d2v_model = gensim.models.doc2vec.Doc2Vec.load('../input/d2v-model/d2v.model')\n\ndef keywords(sentence) :\n\n    doc = nlp(sentence)\n\n    results = ([(X.text, X.label_)[0] for X in doc.ents])\n    return list(set(results))\n\ndef clean_wiki_text(text) :\n    text = re.sub(r'==.+==', '.', text)\n    text = re.sub(r'\\n', ' ', text)\n    text = re.sub(r'\\t', ' ', text)\n    text = re.sub(r'\\[[0-9]+\\]', ' ', text)\n    text = re.sub(r' +', ' ', text)\n    text = re.sub(r'\\. \\.', '.', text)\n    return text\n\ndef content(claim, results) :\n    sentences = []\n    found=[]\n    for i in results:\n        try :\n            current_page = wiki.page(i)\n            if current_page not in found:\n                found.append(current_page)\n        except :\n            continue\n    titles=[i.title for i in found]\n    titles=[i[0] for i in topNSimilar(claim, titles)]\n    for i in found:\n        if i.title not in titles:\n            found.remove(i)\n\n    for i in found:\n        current_content = i.content\n        sentences.extend(tokenize.sent_tokenize(clean_wiki_text(current_content)))\n    return sentences\n\ndef topNSimilar(claim, sentList, n=5) :\n    distList=[]\n    for i in sentList:\n        document_term_matrix = TfidfVectorizer().fit_transform([i, claim])\n        dist = document_term_matrix * document_term_matrix.transpose()\n        distance = dist.toarray()[0][1]\n\n        if len(distList)<=n:\n            distList.append([i, distance])\n        else:\n            distList=sorted(distList, key=lambda x: x[1], reverse=True)\n            if distance>distList[-1][1]:\n                distList.pop()\n                distList.append([i, distance])\n    return sorted(distList, key=lambda x: x[1], reverse=True)\n\ndef topNBert(claim, res) :\n    topNFacts=[i[0] for i in res]\n    topNScore=[cosine_similarity( [wikipedia_model.encode(claim)], [wikipedia_model.encode(i)] )[0][0] for i in topNFacts]\n    topN=zip(topNFacts, topNScore)\n    return [list(i) for i in list(topN)]\n\ndef get_tweet(link) :\n    twt_id = (link.split('/')[-1])\n    twt_id = re.sub(r'\\?.+', '', twt_id)\n    tweetContent = twitterAPI.get_tweet(twt_id)\n    tweetContent = str(tweetContent[0]).split()\n    return (\" \".join(tweetContent))\n\ndef prep(rowitem) :\n    rowitem = nltk.tokenize.word_tokenize(rowitem)\n    rowitem = [i.lower() for i in rowitem if i.isalpha()]\n    rowitem = [ i for i in rowitem if i not in stop_words ]\n    rowitem = ' '.join([ lemma.lemmatize(i) for i in rowitem ])\n    return rowitem\n\ndef detect_news(update, context) :\n    news = update.message.text\n\n    update.message.reply_text(\"Waiting for the output....\")\n\n    if news[:5]=='https' :\n        news = get_tweet(news)\n    textToReply = prediction(news)\n    finalNewsFeatures = getNewsFeatures(news)\n#     finalWikipediaResults = topNBert(news, topNSimilar(news, content(news, keywords(news))))\n#     kws=keywords(news)\n#     cnt=content(news,kws)\n#     tns=topNSimilar(news,cnt)\n#     tnb=topNBert(news,tns)\n    \n    update.message.reply_text(textToReply)\n    update.message.reply_text(\"The news features are : \")\n    for key , val in finalNewsFeatures.items() :\n        update.message.reply_text(key + \" : \" + val)\n#     update.message.reply_text(\"We found these related articles on the web :\")\n#     for i in tnb :\n#         update.message.reply_text(i[0])\n\ndef detect_image(update , context) :\n    \n    photo_file = update.message.photo[-1].get_file()\n    photo_file.download('user_photo.jpg')\n    img_path = 'user_photo.jpg'\n    news = preprocess(get_text_from_image(img_path))\n    \n    if len(news) > 0 : \n        update.message.reply_text(\"Waiting for the output....\")\n        textToReply = prediction(news)\n        finalNewsFeatures = getNewsFeatures(news)\n        finalWikipediaResults = topNBert(news, topNSimilar(news, content(news, keywords(news))))\n        kws=keywords(news)\n        cnt=content(news,kws)\n        tns=topNSimilar(news,cnt)\n        tnb=topNBert(news,tns)\n        update.message.reply_text(textToReply)\n        update.message.reply_text(\"The news features are : \")\n        for key , val in finalNewsFeatures.items():\n            update.message.reply_text(key + \" : \" + val)\n        if len(tnb.values()) == 0 :\n            update.message.reply_text(\"We found no related articles on the web\")\n        else :\n            update.message.reply_text(\"We found these related articles on the web :\")\n            for i in tnb :\n                update.message.reply_text(i[0])          \n    else:\n        update.message.reply_text(\"The model was not able to parse text from the given image\")\n\ndef preprocess(text) :\n    PRETRAINED_MODEL_PATH = './lid.176.bin'\n    model = fasttext.load_model(PRETRAINED_MODEL_PATH)\n    return ' '.join([i  for i in text.split(' ') if len(i) != 1 if '__label__en' in model.predict(i, k=3)[0]])\n\ndef get_text_from_image(img_path) :\n    url = \"https://app.nanonets.com/api/v2/OCR/FullText\"\n    payload={'urls': ['MY_IMAGE_URL']}\n    files=[('file',('FILE_NAME',open(img_path,'rb'),'application/pdf'))]\n    headers = {}\n\n    response = requests.request(\"POST\", url, headers=headers, data=payload, files=files, auth=requests.auth.HTTPBasicAuth('I-yhRSzNQmxj8dfhXKUQVA55Wj_1Sqjy', ''))\n\n    return json.loads(response.text)['results'][0]['page_data'][0]['raw_text']\n\ndef prediction(news) :\n    \n#     test_data = word_tokenize(news)\n#     v1 = d2v_model.infer_vector(test_data)\n#     similar_doc = d2v_model.docvecs.most_similar(positive=[v1])\n#     if similar_doc[0][1]>=0.725 :\n\n    sentences=[news]\n    tokenized = cae_model_tokenizer(sentences, return_tensors=\"np\", padding=\"longest\")\n    outputs = classification_after_embedding_model(tokenized).logits\n    classifications = np.argmax(outputs, axis=1)\n    if classifications[0]==0 :\n        textToReply = \"The given news needs to be verified\"\n    else :\n        textToReply = \"The given news is TRUE\"\n        \n    return textToReply\n\ndef getNewsFeatures(inputText) :\n    finalNewsFeatures = {}\n    results_model1 = clickBaitModel(inputText)[0]\n    if(results_model1['label']=='Clickbait') :\n        finalNewsFeatures.__setitem__('Clickbait probability', str(round((results_model1['score']*100), 2))+\"%\")\n    else :\n        finalNewsFeatures.__setitem__('Clickbait probability', str(round(((1-results_model1['score'])*100), 2))+\"%\")\n    results_model2 = sentimentModel(inputText)[0]\n    finalNewsFeatures.__setitem__('Sentiment', results_model2['label'])\n    results_model3 = biasModel(inputText)[0]\n    if(results_model3['label']=='Biased') :\n        finalNewsFeatures.__setitem__('Biased percentage', str(round((results_model3['score']*100), 2))+\"%\")\n    else :\n        finalNewsFeatures.__setitem__('Biased percentage', str(round(((1-results_model3['score'])*100), 2))+\"%\")\n    results_model4 = Detoxify('original').predict(inputText)\n    finalNewsFeatures.__setitem__('Toxicity percentage', str(round((results_model4['toxicity']*100), 2))+\"%\")\n    finalNewsFeatures.__setitem__('Obscene percentage', str(round((results_model4['obscene']*100), 2))+\"%\")\n    finalNewsFeatures.__setitem__('Insult percentage', str(round((results_model4['insult']*100), 2))+\"%\")\n    finalNewsFeatures.__setitem__('Hatred percentage', str(round((results_model4['identity_attack']*100), 2))+\"%\")\n    finalNewsFeatures.__setitem__('Threat percentage', str(round((results_model4['threat']*100), 2))+\"%\")\n    return finalNewsFeatures","metadata":{"id":"8M_KXUw2ONWR","scrolled":true,"execution":{"iopub.status.busy":"2023-05-20T09:57:07.502171Z","iopub.status.idle":"2023-05-20T09:57:07.502801Z","shell.execute_reply.started":"2023-05-20T09:57:07.502549Z","shell.execute_reply":"2023-05-20T09:57:07.502581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main() :\n    load_models()\n    print(\"All models are loaded\")\n    TOKEN = \"5477065061:AAG9mOQ4W4Wus2nG1MmOSvOaO2Yp3fbAI2s\"\n    updater = Updater(token = TOKEN , use_context=True)\n    dp = updater.dispatcher\n    dp.add_handler(CommandHandler(\"start\", start))\n    dp.add_handler(CommandHandler(\"help\", help_command))\n    dp.add_handler(MessageHandler(Filters.text, detect_news))\n    dp.add_handler(MessageHandler(Filters.photo , detect_image))\n    updater.start_polling()\n    updater.idle()\n\nif __name__ == '__main__':\n    main()","metadata":{"id":"EW0tjQUDhnid","execution":{"iopub.status.busy":"2023-05-20T09:57:07.504375Z","iopub.status.idle":"2023-05-20T09:57:07.504984Z","shell.execute_reply.started":"2023-05-20T09:57:07.504752Z","shell.execute_reply":"2023-05-20T09:57:07.504775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'flax-sentence-embeddings/all_datasets_v3_roberta-large'\nmodel = SentenceTransformer(model_name)\nmodel.to('cuda:0' if torch.cuda.is_available() else 'cpu')\nnltk.download('stopwords')\nnltk.download('omw-1.4')\nnltk.download('punkt')\nnltk.download('wordnet')\nstop_words = set(stopwords.words('english'))\nlemma = WordNetLemmatizer()","metadata":{"id":"i7_Wj6S8qq91","execution":{"iopub.status.busy":"2023-05-20T09:57:07.507090Z","iopub.status.idle":"2023-05-20T09:57:07.507772Z","shell.execute_reply.started":"2023-05-20T09:57:07.507476Z","shell.execute_reply":"2023-05-20T09:57:07.507503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EMB_FILE_PTH = '../input/inputdata/emb_final.csv'\ndf = pd.read_csv(EMB_FILE_PTH)\nembeddings = df.values.tolist()","metadata":{"id":"4Mm3nXwZ0xwA","execution":{"iopub.status.busy":"2023-05-20T09:57:07.510123Z","iopub.status.idle":"2023-05-20T09:57:07.511333Z","shell.execute_reply.started":"2023-05-20T09:57:07.511068Z","shell.execute_reply":"2023-05-20T09:57:07.511101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prep(rowitem):\n    if len(str(rowitem).split()) < 10:\n        return None\n    rowitem = nltk.tokenize.word_tokenize(rowitem)\n    rowitem = [i.lower() for i in rowitem if i.isalpha()]\n    rowitem = [ i for i in rowitem if i not in stop_words ]\n    rowitem = ' '.join([ lemma.lemmatize(i) for i in rowitem ])\n\n    return rowitem","metadata":{"id":"sgjVmSus3bKL","execution":{"iopub.status.busy":"2023-05-20T09:57:07.513377Z","iopub.status.idle":"2023-05-20T09:57:07.514038Z","shell.execute_reply.started":"2023-05-20T09:57:07.513803Z","shell.execute_reply":"2023-05-20T09:57:07.513826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sim(text, embeddings):\n    f = 0\n    t = [prep(text)]\n    if t[0] == None:\n        return 'Too small'\n    sen_embeddings = model.encode(t)\n    for idx, i in enumerate(embeddings) :\n        sim = cosine_similarity(list(np.asarray(i).reshape(1, -1)),list(sen_embeddings))\n        if sim > 0.8:\n            f = 1\n            return ('Present at '+str(idx))\n\n    if not f:\n        return 'Not Present'","metadata":{"execution":{"iopub.status.busy":"2023-05-20T09:57:07.515607Z","iopub.status.idle":"2023-05-20T09:57:07.516324Z","shell.execute_reply.started":"2023-05-20T09:57:07.516094Z","shell.execute_reply":"2023-05-20T09:57:07.516117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = input('Enter text ')\nprint(sim(text, embeddings))","metadata":{"execution":{"iopub.status.busy":"2023-05-20T09:57:07.518263Z","iopub.status.idle":"2023-05-20T09:57:07.518753Z","shell.execute_reply.started":"2023-05-20T09:57:07.518503Z","shell.execute_reply":"2023-05-20T09:57:07.518543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}